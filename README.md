# Motion Dreamer: Realizing Physically Coherent Video Generation through Scene-Aware Motion Reasoning

**Tianshuo Xu<sup>1,\*</sup>, Zhifei Chen<sup>1,\*</sup>, Leyi Wu<sup>1</sup>, Hao Lu<sup>1</sup>, Yuying Chen<sup>2</sup>, Lihui Jiang<sup>2</sup>, Bingbing Liu<sup>2</sup>, Yingcong Chen<sup>1,3,†</sup>**

<sup>1</sup>HKUST(GZ), <sup>2</sup>Noah's Ark Lab, <sup>3</sup>HKUST  

\* Equal Contribution, † Corresponding Author


[![page](https://img.shields.io/badge/page-visit-blue?style=for-the-badge)](https://yuevii.github.io/motion-dreamer/)
[![paper](https://img.shields.io/badge/paper-view-blue?style=for-the-badge)](https://arxiv.org/abs/2412.00547)


## TL;DR

We introduce Motion Dreamer, a two-stage video generation framework that decouples motion reasoning from high-fidelity video synthesis, addressing the challenge of producing physically coherent videos.

## Overview
![overview](static/overview.png "Hover Title")


## TODO

| Tasks |     Status    |
|---------------------------------------|---------------|
| Release the collected driving data     | [ ] |
| Release training/evaluation code       | [ ] |
| Release the official model             | [ ] |

